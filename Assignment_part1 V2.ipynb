{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "A very important aspect of supervised and semi-supervised machine learning is the quality of the labels produced by human labelers. Unfortunately, humans are not perfect and in some cases may even maliciously label things incorrectly. In this assignment, you will evaluate the impact of incorrect labels on a number of different classifiers.\n",
    "\n",
    "We have provided a number of code snippets you can use during this assignment. Feel free to modify them or replace them.\n",
    "\n",
    "\n",
    "## Dataset\n",
    "The dataset you will be using is the [Adult Income dataset](https://archive.ics.uci.edu/ml/datasets/Adult). This dataset was created by Ronny Kohavi and Barry Becker and was used to predict whether a person's income is more/less than 50k USD based on census data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocessing\n",
    "Start by loading and preprocessing the data. Remove NaN values, convert strings to categorical variables and encode the target variable (the string <=50K, >50K in column index 14)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OrdinalEncoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples before removing nan-values: 48842\n",
      "Number of samples after removing nan-values: 45222\n",
      "salary\n",
      "<=50K    34014\n",
      ">50K     11208\n",
      "dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education</th>\n",
       "      <th>education-num</th>\n",
       "      <th>marital-status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>sex</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "      <th>native-country</th>\n",
       "      <th>salary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39</td>\n",
       "      <td>State-gov</td>\n",
       "      <td>77516</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>2174</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50</td>\n",
       "      <td>Self-emp-not-inc</td>\n",
       "      <td>83311</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>38</td>\n",
       "      <td>Private</td>\n",
       "      <td>215646</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>53</td>\n",
       "      <td>Private</td>\n",
       "      <td>234721</td>\n",
       "      <td>11th</td>\n",
       "      <td>7</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Husband</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28</td>\n",
       "      <td>Private</td>\n",
       "      <td>338409</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Wife</td>\n",
       "      <td>Black</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>Cuba</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age         workclass  fnlwgt  education  education-num  \\\n",
       "0   39         State-gov   77516  Bachelors             13   \n",
       "1   50  Self-emp-not-inc   83311  Bachelors             13   \n",
       "2   38           Private  215646    HS-grad              9   \n",
       "3   53           Private  234721       11th              7   \n",
       "4   28           Private  338409  Bachelors             13   \n",
       "\n",
       "       marital-status         occupation   relationship   race     sex  \\\n",
       "0       Never-married       Adm-clerical  Not-in-family  White    Male   \n",
       "1  Married-civ-spouse    Exec-managerial        Husband  White    Male   \n",
       "2            Divorced  Handlers-cleaners  Not-in-family  White    Male   \n",
       "3  Married-civ-spouse  Handlers-cleaners        Husband  Black    Male   \n",
       "4  Married-civ-spouse     Prof-specialty           Wife  Black  Female   \n",
       "\n",
       "   capital-gain  capital-loss  hours-per-week native-country salary  \n",
       "0          2174             0              40  United-States  <=50K  \n",
       "1             0             0              13  United-States  <=50K  \n",
       "2             0             0              40  United-States  <=50K  \n",
       "3             0             0              40  United-States  <=50K  \n",
       "4             0             0              40           Cuba  <=50K  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This can be used to load the dataset\n",
    "data = pd.read_csv(\"adult.csv\", na_values='?')\n",
    "\n",
    "print('Number of samples before removing nan-values: ' + str(data.shape[0]))\n",
    "\n",
    "data.dropna(inplace=True)\n",
    "\n",
    "print('Number of samples after removing nan-values: ' + str(data.shape[0]))\n",
    "\n",
    "data['salary'] = data['salary'].apply(lambda x: x.strip('.')) # Remove trailing dot\n",
    "\n",
    "print(data.groupby('salary').size())\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data classification\n",
    "Choose at least 4 different classifiers and evaluate their performance in predicting the target variable. \n",
    "\n",
    "#### Preprocessing\n",
    "Think about how you are going to encode the categorical variables, normalization, whether you want to use all of the features, feature dimensionality reduction, etc. Justify your choices \n",
    "\n",
    "A good method to apply preprocessing steps is using a Pipeline. Read more about this [here](https://machinelearningmastery.com/columntransformer-for-numerical-and-categorical-data/) and [here](https://medium.com/vickdata/a-simple-guide-to-scikit-learn-pipelines-4ac0d974bdcf). \n",
    "\n",
    "<!-- #### Data visualization\n",
    "Calculate the correlation between different features, including the target variable. Visualize the correlations in a heatmap. A good example of how to do this can be found [here](https://towardsdatascience.com/better-heatmaps-and-correlation-matrix-plots-in-python-41445d0f2bec). \n",
    "\n",
    "Select a features you think will be an important predictor of the target variable and one which is not important. Explain your answers. -->\n",
    "\n",
    "#### Evaluation\n",
    "Use a validation technique from the previous lecture to evaluate the performance of the model. Explain and justify which metrics you used to compare the different models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 12\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlinear_model\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LogisticRegression\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mneighbors\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m KNeighborsClassifier\n\u001b[1;32m---> 12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'matplotlib'"
     ]
    }
   ],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import Normalizer, OrdinalEncoder, LabelEncoder, OneHotEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K nearest score: 0.6318908441269446\n"
     ]
    }
   ],
   "source": [
    "cats = ['workclass', 'education', 'occupation', 'relationship', 'race', 'sex', 'native-country']\n",
    "nums = ['age', 'fnlwgt', 'capital-gain', 'capital-loss', 'hours-per-week']\n",
    "\n",
    "# cats = ['occupation']\n",
    "# nums = ['hours-per-week']\n",
    "\n",
    "# We have not validated that the rest of the data is of sufficient quality.\n",
    "# We only found that salary has trailing dots some times.\n",
    "\n",
    "steps = [\n",
    "    ('cat', OneHotEncoder(), cats), # Maybe try one hot encoding.\n",
    "    ('num', StandardScaler(), nums)      # Other normalizing for the numericals\n",
    "]\n",
    "\n",
    "# Combine steps into a ColumnTransformer\n",
    "ct = ColumnTransformer(steps)\n",
    "\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(data['salary']).astype(int)\n",
    "\n",
    "# show the correlation between different features including target variable\n",
    "def visualize(data, ct):\n",
    "    X = ct.fit_transform(data)\n",
    "    X = pd.DataFrame(X)\n",
    "    X['salary'] = y\n",
    "\n",
    "    plt.matshow(X.corr())\n",
    "    plt.colorbar()\n",
    "    plt.show()\n",
    "\n",
    "# Apply your model to feature array X and labels y\n",
    "def apply_model(model, X, y):    \n",
    "    # Wrap the model and steps into a Pipeline\n",
    "    pipeline = Pipeline(steps=[('t', ct), ('m', model)])\n",
    "    # Evaluate the model and store results\n",
    "    return evaluate_model(X, y, pipeline)\n",
    "\n",
    "# Apply your validation techniques and calculate metrics\n",
    "def evaluate_model(X, y, pipeline):\n",
    "    # X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "    # pipeline.fit(X_train, y_train)      # Fits the model using the transformed data\n",
    "    # s = pipeline.score(X_test, y_test)  # Uses the standard scoring function of the model\n",
    "\n",
    "    cv = cross_val_score(pipeline, X, y, cv=3, scoring='f1', n_jobs=-1)\n",
    "\n",
    "    return np.nanmean(cv)\n",
    "\n",
    "# visualize(data, ct)\n",
    "\n",
    "# These are all untuned, can do some work by setting the parameters for each model.\n",
    "# Tree \n",
    "# print(\"Tree score:\", apply_model(DecisionTreeClassifier(), data, y))\n",
    "\n",
    "# Random forest\n",
    "# print(\"Random Forest score:\", apply_model(RandomForestClassifier(), data, y))\n",
    "\n",
    "# # SVC\n",
    "# print(\"SVC score:\", apply_model(SVC(), data, y))\n",
    "\n",
    "# # K - nearest neighbours\n",
    "print(\"K nearest score:\", apply_model(KNeighborsClassifier(), data, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Federal-gov' 'Local-gov' 'Private' 'Self-emp-inc' 'Self-emp-not-inc'\n",
      " 'State-gov' 'Without-pay']\n",
      "['10th' '11th' '12th' '1st-4th' '5th-6th' '7th-8th' '9th' 'Assoc-acdm'\n",
      " 'Assoc-voc' 'Bachelors' 'Doctorate' 'HS-grad' 'Masters' 'Preschool'\n",
      " 'Prof-school' 'Some-college']\n",
      "['Adm-clerical' 'Armed-Forces' 'Craft-repair' 'Exec-managerial'\n",
      " 'Farming-fishing' 'Handlers-cleaners' 'Machine-op-inspct' 'Other-service'\n",
      " 'Priv-house-serv' 'Prof-specialty' 'Protective-serv' 'Sales'\n",
      " 'Tech-support' 'Transport-moving']\n",
      "['Husband' 'Not-in-family' 'Other-relative' 'Own-child' 'Unmarried' 'Wife']\n",
      "['Amer-Indian-Eskimo' 'Asian-Pac-Islander' 'Black' 'Other' 'White']\n",
      "['Female' 'Male']\n",
      "['Cambodia' 'Canada' 'China' 'Columbia' 'Cuba' 'Dominican-Republic'\n",
      " 'Ecuador' 'El-Salvador' 'England' 'France' 'Germany' 'Greece' 'Guatemala'\n",
      " 'Haiti' 'Holand-Netherlands' 'Honduras' 'Hong' 'Hungary' 'India' 'Iran'\n",
      " 'Ireland' 'Italy' 'Jamaica' 'Japan' 'Laos' 'Mexico' 'Nicaragua'\n",
      " 'Outlying-US(Guam-USVI-etc)' 'Peru' 'Philippines' 'Poland' 'Portugal'\n",
      " 'Puerto-Rico' 'Scotland' 'South' 'Taiwan' 'Thailand' 'Trinadad&Tobago'\n",
      " 'United-States' 'Vietnam' 'Yugoslavia']\n"
     ]
    }
   ],
   "source": [
    "# TODO: Check that all these are valid and/or needed:\n",
    "for cats in ct.transformers_[0][1].categories_:\n",
    "    print(cats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label perturbation\n",
    "To evaluate the impact of faulty labels in a dataset, we will introduce some errors in the labels of our data.\n",
    "\n",
    "\n",
    "#### Preparation\n",
    "Start by creating a method which alters a dataset by selecting a percentage of rows randomly and swaps labels from a 0->1 and 1->0. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Given a label vector, create a new copy where a random fraction of the labels have been flipped.\"\"\"\n",
    "def pertubate(y: np.ndarray, fraction: float) -> np.ndarray:\n",
    "    copy = data.copy()\n",
    "    # Flip fraction*len(data) of the labels in copy\n",
    "    return copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analysis\n",
    "Create a number of new datasets with perturbed labels, for fractions ranging from `0` to `0.5` in increments of `0.1`.\n",
    "\n",
    "Perform the same experiment you did before, which compared the performances of different models except with the new datasets. Repeat your experiment at least 5x for each model and perturbation level and calculate the mean and variance of the scores. Visualize the change in score for different perturbation levels for all of the models in a single plot. \n",
    "\n",
    "State your observations. Is there a change in the performance of the models? Are there some classifiers which are impacted more/less than other classifiers and why is this the case?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations + explanations: max. 400 words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discussion\n",
    "\n",
    "1)  Discuss how you could reduce the impact of wrongly labeled data or correct wrong labels. <br />\n",
    "    max. 400 words\n",
    "\n",
    "\n",
    "\n",
    "    Authors: Youri Arkesteijn, Tim van der Horst and Kevin Chong.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning Workflow\n",
    "\n",
    "From part 1, you will have gone through the entire machine learning workflow which are they following steps:\n",
    "\n",
    "1) Data Loading\n",
    "2) Data Pre-processing\n",
    "3) Machine Learning Model Training\n",
    "4) Machine Learning Model Testing\n",
    "\n",
    "You can see these tasks are very sequential, and need to be done in a serial fashion. \n",
    "\n",
    "As a small perturbation in the actions performed in each of the steps may have a detrimental knock-on effect in the task that comes afterwards.\n",
    "\n",
    "In the final part of Part 1, you will have experienced the effects of performing perturbations to the machine learning model training aspect and the reaction of the machine learning model testing section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 Data Discovery\n",
    "\n",
    "Now we have a some datasets that are related to each other.\n",
    "\n",
    "**Altogether they are the same as the adult dataset used in the part 1 of the assignment.**\n",
    "\n",
    "In this scenario, one can see the utility of the subsets of data can impact the outcome of the task from the previous section.\n",
    "\n",
    "Because the data is split up, we want to be able to re-construct the data through data discovery.\n",
    "\n",
    "As data discovery will allow you to be able to find relations that can be used to reconstruct the entire dataset.\n",
    "\n",
    "Implement a method of your choice to perform the data discovery to be able to recover the entire dataset from part 1 of the assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discovery_algorithm(datasets: list, threshold: float) -> list:\n",
    "    \"\"\"Function should be able to perform data discovery to find related datasets\n",
    "    Possible Input: List of datasets\n",
    "    Output: List of pairs of related datasets\n",
    "    \"\"\"\n",
    "    similar_datasets = []\n",
    "    for i, dataset1 in enumerate(datasets):\n",
    "        for j, dataset2 in enumerate(datasets):\n",
    "            if i == j or i > j:\n",
    "                continue\n",
    "            col1 = dataset1.columns.str.lower()\n",
    "            col2 = dataset2.columns.str.lower()\n",
    "            #calculate intersection\n",
    "            intersection = col1.intersection(col2)\n",
    "            union = col1.union(col2)\n",
    "            #calculate similarity\n",
    "            similarity = len(intersection)/len(union)\n",
    "            # if similarity >= threshold:\n",
    "            similar_datasets.append((i, j, similarity, intersection))\n",
    "\n",
    "    similar_datasets.sort(key=lambda x: x[2], reverse=True)\n",
    "    return similar_datasets\n",
    "\n",
    "def merge_columns(merged_dataset: pd.DataFrame, intersection: list) -> pd.DataFrame:\n",
    "    for column in intersection:\n",
    "        merged_dataset[column] = merged_dataset.apply(lambda row: merge_column(row[f\"{column}_df1\"], row[f\"{column}_df2\"], column), axis=1)\n",
    "\n",
    "    merged_dataset.drop(columns=[f\"{col}_df1\" for col in intersection] + [f\"{col}_df2\" for col in intersection], inplace=True)\n",
    "    return merged_dataset\n",
    "\n",
    "def merge_column(val1, val2, column):\n",
    "    if pd.isna(val1):\n",
    "        return val2\n",
    "    elif pd.isna(val2):\n",
    "        return val1\n",
    "    elif val1 == val2:\n",
    "        return val1\n",
    "    else:\n",
    "        return None  # or some default value\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'workclass'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_16968\\2429863821.py\u001b[0m in \u001b[0;36m?\u001b[1;34m()\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m \u001b[1;31m## Do some testing using different degrees of relations on the downstream task.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m \u001b[1;31m##\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mread_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\kaspe\\Documents\\repositories\\dme-1\\.venv\\lib\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[0;32m    166\u001b[0m             \u001b[0mvalidate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidate\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    167\u001b[0m             \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    168\u001b[0m         )\n\u001b[0;32m    169\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 170\u001b[1;33m         op = _MergeOperation(\n\u001b[0m\u001b[0;32m    171\u001b[0m             \u001b[0mleft_df\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    172\u001b[0m             \u001b[0mright_df\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    173\u001b[0m             \u001b[0mhow\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mhow\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\kaspe\\Documents\\repositories\\dme-1\\.venv\\lib\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, indicator, validate)\u001b[0m\n\u001b[0;32m    790\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mright_join_keys\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    791\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin_names\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    792\u001b[0m             \u001b[0mleft_drop\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    793\u001b[0m             \u001b[0mright_drop\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 794\u001b[1;33m         ) = self._get_merge_keys()\n\u001b[0m\u001b[0;32m    795\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    796\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mleft_drop\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    797\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mleft\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mleft\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_drop_labels_or_levels\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mleft_drop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\kaspe\\Documents\\repositories\\dme-1\\.venv\\lib\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1293\u001b[0m                         \u001b[1;31m# Then we're either Hashable or a wrong-length arraylike,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1294\u001b[0m                         \u001b[1;31m#  the latter of which will raise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1295\u001b[0m                         \u001b[0mrk\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mHashable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1296\u001b[0m                         \u001b[1;32mif\u001b[0m \u001b[0mrk\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1297\u001b[1;33m                             \u001b[0mright_keys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mright\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_label_or_level_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1298\u001b[0m                         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1299\u001b[0m                             \u001b[1;31m# work-around for merge_asof(right_index=True)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1300\u001b[0m                             \u001b[0mright_keys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mright\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\kaspe\\Documents\\repositories\\dme-1\\.venv\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1907\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mxs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mother_axes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1908\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_level_reference\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1909\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_level_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1910\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1911\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1912\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1913\u001b[0m         \u001b[1;31m# Check for duplicates\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1914\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'workclass'"
     ]
    }
   ],
   "source": [
    "## Do some testing using different degrees of relations on the downstream task.\n",
    "##\n",
    "\n",
    "def read_data(file_name):\n",
    "    df = pd.read_csv(file_name, na_values='?').iloc[: , 1:]\n",
    "    if 'salary' in df.columns:\n",
    "        df['salary'] = df['salary'].str.replace('.', '')\n",
    "    return df\n",
    "\n",
    "dataset_names = [\"adult_split/data_adult_0_1.csv\", \"adult_split/data_adult_0_2.csv\", \"adult_split/data_adult_1_1.csv\", \"adult_split/data_adult_1_2.csv\", \"adult_split/data_adult_2_1.csv\", \"adult_split/data_adult_2_2.csv\"]\n",
    "\n",
    "select_columns = [\"age\",\"workclass\",\"fnlwgt\",\"education\",\"education-num\",\"marital-status\",\"occupation\",\"relationship\",\"race\",\"sex\",\"capital-gain\",\"capital-loss\",\"hours-per-week\",\"native-country\",\"salary\"]\n",
    "datasets = [read_data(name) for name in dataset_names]\n",
    "\n",
    "\n",
    "#merge similar datasets\n",
    "while len(datasets) > 1:\n",
    "    similar_datasets = discovery_algorithm(datasets, 0.5)\n",
    "    di, dj, similarity, intersection = similar_datasets.pop(0)\n",
    "    join_on  = [col for col in intersection if col in ['first_name', 'last_names', 'race', 'sex']]\n",
    "    merged_dataset = pd.merge(datasets[di], datasets[dj], how='outer', on=['first_name', 'last_names'], suffixes=('_df1', '_df2'))\n",
    "    merged_dataset = merge_columns(merged_dataset, list(set(intersection) - set(['first_name', 'last_names'])))\n",
    "    remove_indices = [di,dj]\n",
    "    datasets = [i for j, i in enumerate(datasets) if j not in remove_indices]\n",
    "    datasets.append(merged_dataset)\n",
    "\n",
    "    take_columns = [col for col in select_columns if col in merged_dataset.columns.str.lower()]\n",
    "    final_dataset = merged_dataset[take_columns]\n",
    "    final_dataset.to_csv(f\"adult_merged/adult_merged_{5-len(datasets)}.csv\", index=False)\n",
    "\n",
    "\n",
    "\n",
    "#check overlap with actual data\n",
    "pd.merge(final_dataset, data, how='inner', on=select_columns)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'workclass'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_16968\\908721758.py\u001b[0m in \u001b[0;36m?\u001b[1;34m()\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m \u001b[0mdatasets_random\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mread_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdataset_names\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;32mwhile\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdatasets_random\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdatasets_random\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m6\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\kaspe\\Documents\\repositories\\dme-1\\.venv\\lib\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[0;32m    166\u001b[0m             \u001b[0mvalidate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidate\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    167\u001b[0m             \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    168\u001b[0m         )\n\u001b[0;32m    169\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 170\u001b[1;33m         op = _MergeOperation(\n\u001b[0m\u001b[0;32m    171\u001b[0m             \u001b[0mleft_df\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    172\u001b[0m             \u001b[0mright_df\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    173\u001b[0m             \u001b[0mhow\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mhow\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\kaspe\\Documents\\repositories\\dme-1\\.venv\\lib\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, indicator, validate)\u001b[0m\n\u001b[0;32m    790\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mright_join_keys\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    791\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin_names\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    792\u001b[0m             \u001b[0mleft_drop\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    793\u001b[0m             \u001b[0mright_drop\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 794\u001b[1;33m         ) = self._get_merge_keys()\n\u001b[0m\u001b[0;32m    795\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    796\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mleft_drop\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    797\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mleft\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mleft\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_drop_labels_or_levels\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mleft_drop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\kaspe\\Documents\\repositories\\dme-1\\.venv\\lib\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1293\u001b[0m                         \u001b[1;31m# Then we're either Hashable or a wrong-length arraylike,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1294\u001b[0m                         \u001b[1;31m#  the latter of which will raise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1295\u001b[0m                         \u001b[0mrk\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mHashable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1296\u001b[0m                         \u001b[1;32mif\u001b[0m \u001b[0mrk\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1297\u001b[1;33m                             \u001b[0mright_keys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mright\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_label_or_level_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1298\u001b[0m                         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1299\u001b[0m                             \u001b[1;31m# work-around for merge_asof(right_index=True)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1300\u001b[0m                             \u001b[0mright_keys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mright\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\kaspe\\Documents\\repositories\\dme-1\\.venv\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1907\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mxs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mother_axes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1908\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_level_reference\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1909\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_level_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1910\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1911\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1912\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1913\u001b[0m         \u001b[1;31m# Check for duplicates\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1914\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'workclass'"
     ]
    }
   ],
   "source": [
    "datasets_random = [read_data(name) for name in dataset_names]\n",
    "\n",
    "while len(datasets_random) > 1:\n",
    "    if len(datasets_random) == 6:\n",
    "        di = np.random.choice([1,2,5])\n",
    "        dj = np.random.choice([j for j in range(len(datasets_random)) if j != di])\n",
    "    else:\n",
    "        di, dj = np.random.choice(range(len(datasets_random)), 2, replace=False)\n",
    "    col1 = datasets_random[di].columns.str.lower()\n",
    "    col2 = datasets_random[dj].columns.str.lower()\n",
    "    #calculate intersection\n",
    "    intersection = col1.intersection(col2)\n",
    "    join_on  = [col for col in intersection if col in ['first_name', 'last_names', 'race', 'sex']]\n",
    "    merged_dataset = pd.merge(datasets_random[di], datasets_random[dj], how='outer', on=['first_name', 'last_names'], suffixes=('_df1', '_df2'))\n",
    "    merged_dataset = merge_columns(merged_dataset, list(set(intersection) - set(['first_name', 'last_names'])))\n",
    "    remove_indices = [di,dj]\n",
    "    datasets_random = [i for j, i in enumerate(datasets_random) if j not in remove_indices]\n",
    "    datasets_random.append(merged_dataset)\n",
    "\n",
    "    take_columns = [col for col in select_columns if col in merged_dataset.columns.str.lower()]\n",
    "    final_dataset = merged_dataset[take_columns]\n",
    "    final_dataset.to_csv(f\"adult_merged/adult_random_{5-len(datasets_random)}.csv\", index=False)\n",
    "\n",
    "pd.merge(final_dataset, data, how='inner', on=select_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "\n",
    "Following the same workflow as Part 1 of the assignment, you will need to perform the steps once again.\n",
    "This means that with the difference in setting, there has now been a change in the data loading portion of the workflow. \n",
    "\n",
    "<!-- While performing data discovery, one can check how having more data of different relations can effect the outcome of the downstream task. -->\n",
    "\n",
    "As you perform the act of data discovery you will be piecing the data back together one by one by finding the relation between the datasets.\n",
    "\n",
    "As you piece the data back together, the entire dataset will be available in varying portions.\n",
    "\n",
    "Then using the different portions of available data, fit the models that were used previously, and examine the results when testing on the appropriate data from the testing samples.\n",
    "\n",
    "*The dataset from part 1 can be treated as the groundtruth, so you can try and random sample from that dataset to produce the testing samples that can be used in this part.* \n",
    "\n",
    "***\n",
    "\n",
    "One can also evaluate on the effect of incorrectness of data discovery.\n",
    "\n",
    "Suppose what you have done for data discovery is correct, this means the relations that are found are correct.\n",
    "\n",
    "What if you perform a perturbation on the relations between the files? \n",
    "\n",
    "This would mean the dataset will be incorrectly joined, do you think there will be an impact on the outcome of the model that will be trained and then tested on the groudtruth dataset?\n",
    "\n",
    "What would the effect be on the downstream tasks as mentioned in the machine learning workflow such as data pre-processing, machine learning model training and testing?\n",
    "\n",
    "This can be evaluated in the same way as above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(model, train_x, train_y, test_x, test_y, ct):\n",
    "    cats = [c for c in ['workclass', 'education', 'occupation', 'relationship', 'race', 'sex', 'native-country'] if c in train_x.columns.str.lower()]\n",
    "    nums = [c for c in ['age', 'fnlwgt', 'capital-gain', 'capital-loss', 'hours-per-week'] if c in train_x.columns.str.lower()]\n",
    "\n",
    "    steps = [\n",
    "        ('cat', OneHotEncoder(handle_unknown=\"ignore\"), cats), # Maybe try one hot encoding.\n",
    "        ('num', StandardScaler(), nums)      # Other normalizing for the numericals\n",
    "    ]\n",
    "\n",
    "    # Combine steps into a ColumnTransformer\n",
    "    ct = ColumnTransformer(steps)\n",
    "\n",
    "    pipeline = Pipeline(steps=[('t', ct), ('m', model)])\n",
    "    pipeline.fit(train_x, train_y)      # Fits the model using the transformed data\n",
    "    s = pipeline.score(test_x, test_y)  # Uses the standard scoring function of the model\n",
    "    return s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K nearest score: 0.6747254367214565\n",
      "Random Forest score: 0.7408417483599912\n",
      "SVC score: 0.7561730670008108\n",
      "Tree score: 0.7481388663669197\n",
      "K nearest score: 0.8648927544777769\n",
      "Random Forest score: 0.9419178889953563\n",
      "SVC score: 0.8609862165548758\n",
      "Tree score: 0.9248175720498267\n",
      "K nearest score: 0.8771283260853542\n",
      "Random Forest score: 0.9882803862312965\n",
      "SVC score: 0.8627552148595857\n",
      "Tree score: 0.9845212648337879\n"
     ]
    }
   ],
   "source": [
    "## Do some testing on performing various degrees of incorrect data discovery.\n",
    "##\n",
    "le = LabelEncoder()\n",
    "\n",
    "test_data = pd.read_csv(\"adult.csv\", na_values='?').dropna()\n",
    "test_data['salary'] = test_data['salary'].apply(lambda x: x.strip('.'))\n",
    "test_data = test_data.sample(frac=0.3).reset_index(drop=True)\n",
    "\n",
    "for i in range(2, 5):\n",
    "    data = pd.read_csv(f\"adult_merged/adult_merged_{i}.csv\", na_values='?')\n",
    "    data.dropna(inplace=True)\n",
    "    data['salary'] = data['salary'].apply(lambda x: x.strip('.')) # Remove trailing dot\n",
    "\n",
    "    train_x = data.drop(columns=['salary'])\n",
    "    train_y = le.fit_transform(data['salary']).astype(int)\n",
    "    test_x = test_data[train_x.columns.str.lower()]\n",
    "    test_y = le.fit_transform(test_data['salary']).astype(int)\n",
    "\n",
    "    print(f\"{i} merged K nearest score:\", run_model(KNeighborsClassifier(n_neighbors=min(train_x.shape[0],5)), train_x, train_y, test_x, test_y, ct))\n",
    "    print(f\"{i} merged Random Forest score:\", run_model(RandomForestClassifier(), train_x, train_y, test_x, test_y, ct))\n",
    "    print(f\"{i} merged SVC score:\", run_model(SVC(), train_x, train_y, test_x, test_y, ct))\n",
    "    print(f\"{i} merged Tree score:\", run_model(DecisionTreeClassifier(), train_x, train_y, test_x, test_y, ct))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 random K nearest score: 0.8362202402889364\n",
      "2 random Random Forest score: 0.8965135991744675\n",
      "2 random SVC score: 0.855679221640746\n",
      "2 random Tree score: 0.8687992924006781\n",
      "3 random K nearest score: 0.8363676568143289\n",
      "3 random Random Forest score: 0.89725068180143\n",
      "3 random SVC score: 0.8561951794796197\n",
      "3 random Tree score: 0.8710105402815655\n",
      "4 random K nearest score: 0.8761701186703029\n",
      "4 random Random Forest score: 0.9883540944939928\n",
      "4 random SVC score: 0.864008255325422\n",
      "4 random Tree score: 0.9859217218250166\n"
     ]
    }
   ],
   "source": [
    "for i in range(2, 5):\n",
    "    data = pd.read_csv(f\"adult_merged/adult_random_{i}.csv\", na_values='?')\n",
    "    data.dropna(inplace=True)\n",
    "    data['salary'] = data['salary'].apply(lambda x: x.strip('.')) # Remove trailing dot\n",
    "\n",
    "    train_x = data.drop(columns=['salary'])\n",
    "    train_y = le.fit_transform(data['salary']).astype(int)\n",
    "    test_x = test_data[train_x.columns.str.lower()]\n",
    "    test_y = le.fit_transform(test_data['salary']).astype(int)\n",
    "\n",
    "    print(f\"{i} random K nearest score:\", run_model(KNeighborsClassifier(n_neighbors=min(train_x.shape[0],5)), train_x, train_y, test_x, test_y, ct))\n",
    "    print(f\"{i} random Random Forest score:\", run_model(RandomForestClassifier(), train_x, train_y, test_x, test_y, ct))\n",
    "    print(f\"{i} random SVC score:\", run_model(SVC(), train_x, train_y, test_x, test_y, ct))\n",
    "    print(f\"{i} random Tree score:\", run_model(DecisionTreeClassifier(), train_x, train_y, test_x, test_y, ct))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussions\n",
    "\n",
    "1)  Discuss the different effects of the results of the data discovery results on various downstream tasks in the machine learning workflow.\n",
    "    As stated previously, this is effecting the data loading portion of the ML workflow.\n",
    "\n",
    "2)  Discuss also what aspects need to be considered when performing data discovery and evaluating the results of data discovery.\n",
    "\n",
    "Max. 400 words\n",
    "\n",
    "The perturbations that performed for part 1 and 2 are all data quality issues.\n",
    "\n",
    "3) Discuss on the effects of data quality and how you may attempt to identify and solve these issues?\n",
    "\n",
    "<!-- For the set of considerations that you have outlined for the choice of data discovery methods, choose one and identify under this new constraint, how would you identify and resolve this problem? -->\n",
    "\n",
    "Max 400 words"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
